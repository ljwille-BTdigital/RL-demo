{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from Yourness Mansar \n",
    "\n",
    "Original Code: https://github.com/CVxTz/RL\n",
    "Article discussing PPO: https://towardsdatascience.com/learning-to-play-cartpole-and-lunarlander-with-proximal-policy-optimization-dacbd6045417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "\n",
    "import time\n",
    "\n",
    "from model import (\n",
    "    PolicyNetwork,\n",
    "    ValueNetwork,\n",
    "    device,\n",
    "    train_value_network,\n",
    "    train_policy_network,\n",
    ")\n",
    "from replay import Episode, History\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):   \n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "    # Update the window after each episode\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(195, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(195, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    env_name=\"CartPole-v1\",\n",
    "    reward_scale=20.0,\n",
    "    clip=0.2,\n",
    "    log_dir=\"../logs\",\n",
    "    learning_rate=0.001,\n",
    "    state_scale=1.0):\n",
    "    \n",
    "    title = env_name\n",
    "    final = []\n",
    "    env = gym.make(env_name)\n",
    "    observation = env.reset()\n",
    "\n",
    "    n_actions = env.action_space.n\n",
    "    feature_dim = observation.size\n",
    "\n",
    "    value_model = ValueNetwork(in_dim=feature_dim).to(device)\n",
    "    value_optimizer = optim.Adam(value_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    policy_model = PolicyNetwork(in_dim=feature_dim, n=n_actions).to(device)\n",
    "    policy_optimizer = optim.Adam(policy_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    n_epoch = 1\n",
    "\n",
    "    max_episodes = 20\n",
    "    max_timesteps = 400\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    max_iterations = 15\n",
    "\n",
    "    history = History()\n",
    "\n",
    "    epoch_ite = 0\n",
    "    episode_ite = 0\n",
    "\n",
    "    for ite in range(max_iterations):\n",
    "\n",
    "        for episode_i in range(max_episodes):\n",
    "\n",
    "            observation = env.reset()\n",
    "            episode = Episode()\n",
    "            total = 0\n",
    "\n",
    "            for timestep in range(max_timesteps):\n",
    "\n",
    "                action, log_probability = policy_model.sample_action(observation / state_scale)\n",
    "                \n",
    "                value = value_model.state_value(observation / state_scale)\n",
    "\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                \n",
    "                total += reward\n",
    "                \n",
    "                episode.append(\n",
    "                    observation=observation / state_scale,\n",
    "                    action=action,\n",
    "                    reward=reward,\n",
    "                    value=value,\n",
    "                    log_probability=log_probability,\n",
    "                    reward_scale=reward_scale,\n",
    "                )\n",
    "\n",
    "                observation = new_observation\n",
    "\n",
    "                if done:\n",
    "                    episode.end_episode(last_value=0)\n",
    "                    break\n",
    "\n",
    "                if timestep == max_timesteps - 1:\n",
    "                    value = value_model.state_value(observation / state_scale)\n",
    "                    episode.end_episode(last_value=value)\n",
    "\n",
    "            episode_ite += 1\n",
    "            final.append(total)\n",
    "            plot_res(final, title)\n",
    "\n",
    "            history.add_episode(episode)\n",
    "\n",
    "        history.build_dataset()\n",
    "        data_loader = DataLoader(history, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        policy_loss = train_policy_network(\n",
    "            policy_model, policy_optimizer, data_loader, epochs=n_epoch, clip=clip\n",
    "        )\n",
    "\n",
    "        value_loss = train_value_network(\n",
    "            value_model, value_optimizer, data_loader, epochs=n_epoch\n",
    "        )\n",
    "\n",
    "        for p_l, v_l in zip(policy_loss, value_loss):\n",
    "            epoch_ite += 1\n",
    "\n",
    "\n",
    "        history.free_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
